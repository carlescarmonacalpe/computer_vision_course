{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignorar sklearn warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='skimage')\n",
    "\n",
    "# Librería de ámbito matemático, científico y de ingeniería. En nuestro caso, solo usamos la convolución.\n",
    "import scipy.io\n",
    "\n",
    "# Interfaz para HDF5. Solo necesario para cargar las anotaciones del dataset.\n",
    "import hdf5storage\n",
    "\n",
    "# Librería para la computación científica. En nuestro caso, la usamos por la representación de datos y hacer calculas de forma\n",
    "# rápida.\n",
    "import numpy as np\n",
    "\n",
    "# Librerías de visión por computador\n",
    "# OpenCV: Generalista.\n",
    "# Skimage: Generalista.\n",
    "# Dlib: Menos funcionalidades, muy rápido e implementaciones muy interesantes.\n",
    "import dlib\n",
    "import cv2\n",
    "from skimage import io\n",
    "from skimage import color\n",
    "from skimage import filters\n",
    "from skimage import util\n",
    "from skimage import transform\n",
    "from skimage import feature\n",
    "\n",
    "# Librerías para gráficos en 2d/3d. En nuestro caso para mostrar las imágenes y gráficos en general.\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Necesario para visualizar los resultados en la misma página.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comprender mejor las explicaciones de la presentación procederemos a entrenar un detector paso por paso. Recomiendo entender lo que está pasando, pero intentar no perder tiempo en detalles insignificantes (si no es que os motiva mucho!). La mayoría de detectores se entrenan usando procesos end2end donde no hay que hacer más que alimentar una implementación ya hecha con datos de entrada, y/o tunear detectores ya existentes para resolver nuestro caso concreto.\n",
    "\n",
    "A continuación vamos a programar nuestro primer detector como profundización de los conceptos teóricos, y en las siguientes secciones veremos cómo entrenar nuestro detector de forma semiautomática.\n",
    "\n",
    "# Entrenando un detector de caras de forma manual\n",
    "\n",
    "## Obtener un set de muestras positivas y negativas\n",
    "\n",
    "Para obtener nuestro set usaremos una base de datos que ya tiene \"anotadas\" las caras y que encontramos disponible en nuestra carpeta de recursos.\n",
    "\n",
    "Ruta:\n",
    "__resources\\face_dataset__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las anotaciones del dataset.\n",
    "anotaciones_path = \"resources/session2/face_dataset/anno.mat\"\n",
    "anotaciones = hdf5storage.loadmat(anotaciones_path)\n",
    "anotaciones = anotaciones[\"anno\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructura de las anotaciones. Esta varia mucha según que de donde proviene el dataset.\n",
    "print(\"Dimensiones: \", anotaciones.shape)\n",
    "print(\"\\nEjemplo de datos de una imagen:\")\n",
    "print(\"-\"*55)\n",
    "print(\"Nombre del fichero: \", anotaciones[0][0][0])\n",
    "print(\"Bboxes:\\n %s\" % anotaciones[0][1][0][0])\n",
    "print(\"Posición de la cara [X axis, Y axis, Z axis]:\\n %s\" % anotaciones[0][2][0][0])\n",
    "print(\"Landmarks:\\n %s\" % anotaciones[0][3][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero de muestras\n",
    "print(\"Numero de imagenes:\", len(anotaciones))\n",
    "print(\"Numero de caras anotadas:\", np.sum([len(anotacion[1][0]) for anotacion in anotaciones]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # Generar números aleatorios\n",
    "\n",
    "# Seleccionar una imagen de forma aleatoria\n",
    "random_image = random.randint(0, (len(anotaciones)))\n",
    "filename = anotaciones[random_image][0][0][0]\n",
    "bboxs = anotaciones[random_image][1][0]\n",
    "landing_marks = anotaciones[random_image][3][0]\n",
    "\n",
    "# Estructura\n",
    "f, (ax0, ax1) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Abrir la imágen\n",
    "img = io.imread(\"resources/session2/face_dataset/\" + filename)\n",
    "ax0.imshow(img);\n",
    "ax0.set_title(\"Imagen sin anotar\")\n",
    "\n",
    "# Dibujar la información disponible\n",
    "for bbox, bbox_landing_marks in zip(bboxs, landing_marks):\n",
    "    # Rectangle\n",
    "    (x1, y1), (x2, y2) = bbox.astype(int)\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "    # Landmarks\n",
    "    for landmark in bbox_landing_marks:\n",
    "        x, y = landmark.astype(int)\n",
    "        cv2.circle(img, (x, y) , 7, (255, 0, 0), -1)\n",
    "        \n",
    "# Mostrar la imagen\n",
    "ax1.imshow(img);\n",
    "ax1.set_title(\"Imagen anotada\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muestras positivas\n",
    "Ahora que ya conocemos un poco mejor la estructura de datos de las anotaciones, procedemos a \"recortar\" solo los fragmentos de imagen que representan caras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples_positivos = 0\n",
    "tamaño_redimensionar = (75, 80)\n",
    "\n",
    "for imagen in anotaciones:\n",
    "    \n",
    "    # Obtener información\n",
    "    nombre_imagen = imagen[0][0][0]\n",
    "    bboxs = imagen[1][0]\n",
    "    \n",
    "    # Cargar imagen\n",
    "    path = \"resources/session2/face_dataset/\" + nombre_imagen\n",
    "    img = io.imread(path)\n",
    "    # Mensaje debug\n",
    "    print(\"Imagen actual:\", path)\n",
    "    print(\"Número de bbox:\", len(bboxs))\n",
    "    \n",
    "    for bbox in bboxs:\n",
    "        # Seleccionar los puntos\n",
    "        (x1, y1), (x2, y2) = bbox.astype(int)\n",
    "        # Recortar la imagen\n",
    "        recorte = img[y1:y2, x1:x2, :]\n",
    "        # Redimensionar todas al mismo tamaño\n",
    "        recorte = transform.resize(recorte, tamaño_redimensionar, mode='constant')\n",
    "        # Guardar el recorte\n",
    "        filename = \"resources/session2/our_face_dataset/positive_samples/%s.jpg\" % (str(num_samples_positivos)) \n",
    "        io.imsave(filename, recorte)\n",
    "        \n",
    "        num_samples_positivos +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Muestras negativas\n",
    "Ahora lo mismo para la muestras negativas, nuestro objetivo es conseguir recortes de cosas que no sean caras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cogemos 4 veces el numero de samples positivos\n",
    "num_samples_negativos = num_samples_positivos * 4\n",
    "num_imagenes = len(anotaciones)\n",
    "muestras_por_imagen = int(np.ceil(num_samples_negativos / num_imagenes))\n",
    "\n",
    "alto_recorte, ancho_recorte = tamaño_redimensionar\n",
    "print(\"Alto recorte: %d, Ancho recorte: %d\" % (alto_recorte, ancho_recorte))\n",
    "\n",
    "for imagen in anotaciones:\n",
    "\n",
    "    # Cargar imagen\n",
    "    nombre_imagen = imagen[0][0][0]\n",
    "    path = \"resources/session2/face_dataset/\" + nombre_imagen\n",
    "    img = io.imread(path)\n",
    "    \n",
    "    print(\"Imagen actual: %s, dimensiones: %s\" % (path, str(img.shape)))\n",
    "    alto, ancho, canales = img.shape\n",
    "        \n",
    "    # Extraer muestras\n",
    "    for num_muestra in range(muestras_por_imagen):\n",
    "          \n",
    "        # Punto aleatorio dentro de la imagen\n",
    "        y1 = random.randint(0, img.shape[0])\n",
    "        x1 = random.randint(0, img.shape[1])\n",
    "        \n",
    "        while ((y1 + alto_recorte >= alto) \n",
    "               or (x1 + ancho_recorte >= ancho)):\n",
    "            x1 = random.randint(0, (img.shape[0]))\n",
    "            y1 = random.randint(0, (img.shape[1]))\n",
    "            \n",
    "        x2, y2 = x1 + ancho_recorte, y1 + alto_recorte\n",
    "        \n",
    "        # Recortamos\n",
    "        recorte = img[y1:y2, x1:x2, :]\n",
    "        print(\"\\tRecorte aleatorio: (%d, %d), (%d, %d)\" % (x1, y1, x2, y2))\n",
    "\n",
    "        # Guardamos el resultado\n",
    "        filename = \"resources/session2/our_face_dataset/negative_samples/%s.jpg\" % str(num_samples_negativos)\n",
    "        io.imsave(filename, recorte)\n",
    "        \n",
    "        num_samples_negativos -= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Anotaciones__\n",
    "* ¿Cuál es la imagen con más bbox?\n",
    "* Hay un error al generar las muestras negativas, ¿podrías decir cuál es?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcular el descriptor Histogram of Gradient para todas las muestras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob # Listado de directorios\n",
    "\n",
    "num_samples_negativos = muestras_por_imagen * num_imagenes\n",
    "total_samples = num_samples_positivos + num_samples_negativos\n",
    "\n",
    "print(\"Muestras positivas: %d, Muestras negativas: %d , Total muestras: %d\" % \n",
    "      (num_samples_positivos, num_samples_negativos, total_samples))\n",
    "\n",
    "tamaño_descriptor = 2592\n",
    "descriptores = np.zeros((total_samples, tamaño_descriptor))\n",
    "\n",
    "actual_sample = 0\n",
    "## Calcular HOG para todas las muestras positivas\n",
    "nombre_carpeta = \"resources/session2/our_face_dataset/positive_samples/\"\n",
    "for filename in glob.glob(nombre_carpeta + '*.jpg'):\n",
    "    # Cargar muestra\n",
    "    image = io.imread(filename,  as_grey=True)\n",
    "    \n",
    "    if actual_sample % 200 == 0:\n",
    "        print(\"Calculando descriptor de la imagen %d/%d\" % (actual_sample, total_samples))\n",
    "\n",
    "    # Calcular descriptor HOG\n",
    "    descriptor = feature.hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2),\n",
    "                             transform_sqrt=True, visualise=False, block_norm=\"L2-Hys\")\n",
    "    descriptores[actual_sample, :] = descriptor\n",
    "    actual_sample += 1\n",
    "\n",
    "## Calcular HOG para todas las muestras negativas\n",
    "nombre_carpeta = \"resources/session2/our_face_dataset/negative_samples/\"\n",
    "for filename in glob.glob(nombre_carpeta + '*.jpg'):\n",
    "    image = io.imread(filename,  as_grey=True)\n",
    "    \n",
    "    if actual_sample % 200 == 0:\n",
    "        print(\"Calculando descriptor de la imagen %d/%d\" % (actual_sample, total_samples))\n",
    "    \n",
    "    descriptor = feature.hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2),\n",
    "                             transform_sqrt=True, visualise=False, block_norm=\"L2-Hys\")\n",
    "    descriptores[actual_sample, :] = descriptor\n",
    "    actual_sample += 1\n",
    "print(\"Calculando descriptor de la imagen %d/%d\" % (actual_sample, total_samples))\n",
    "\n",
    "# Etiquetas\n",
    "labels = np.zeros((descriptores.shape[0]))\n",
    "labels[0:num_samples_positivos] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preguntas__\n",
    "* De que depende el tamaño del descriptor HoG?\n",
    "* Que significan los parametros de la función feature.hog()?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizar los datos\n",
    "Escalamos los datos usando la Clase StandardScaler y dividimos nuestros datos en test, train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"Dimensiones descriptor: %s\" % (str(descriptores.shape)))\n",
    "print(\"Dimensiones labels: %s\" % (str(labels.shape)))\n",
    "\n",
    "# Normalizamos los valores\n",
    "print(\"Ejemplo antes de normalizar: %s\" % (str(descriptores[200,:])))\n",
    "scaler = StandardScaler().fit(descriptores)\n",
    "descriptors_scaled = scaler.transform(descriptores)\n",
    "print(\"Ejemplo después de normalizar: %s\" % (str(descriptors_scaled[200,:])))\n",
    "\n",
    "# Dividimos el dataset en dos partes entrenamiento/test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    descriptors_scaled, labels, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar el clasificador binario\n",
    "Ahora entrenaremos el clasificador SVM (support vector machine) que he explicado en los slides, para ello usaremos basicamente la clase SVC que implementa el clasificador SVM, y luego la clase GridSearchCv que nos permitira optimizar los parametros de nuestro clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Esto es un CTRL-C + CTRL-V de los tutorial de Sklearn: \n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4, 1e-5],\n",
    "                     'C': [1, 10, 100, 1000]}]#,\n",
    "                    #{'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score, n_jobs=4)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preguntas__\n",
    "* ¿Qué parámetros son los buenos?\n",
    "* ¿Qué significa recall y precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primera prueba del clasificador de caras\n",
    "A continuación una pequeña prueba para ver si tiene más o menos sentido lo que hemos hecho hasta ahora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar una lista de imágenes detro de un directorio\n",
    "nombre_carpeta = \"resources/session2/our_face_dataset/test/\"\n",
    "lista_imagenes =glob.glob(nombre_carpeta + '*.jpg')\n",
    "\n",
    "# Generamos una figura con múltiples imágenes\n",
    "fig=plt.figure(figsize=(2, 5))\n",
    "fig.set_size_inches(18.5, 10.5, forward=True)\n",
    "\n",
    "for i in range(1, len(lista_imagenes) + 1):\n",
    "    \n",
    "    # Cargar imagen en grayscale y reescalar\n",
    "    image = io.imread(lista_imagenes[i-1], as_grey=True)\n",
    "    image = transform.resize(image, tamaño_redimensionar, mode='constant')\n",
    "    \n",
    "    # Mostramos la imagen.\n",
    "    ax1 = fig.add_subplot(2, 5, i)\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    \n",
    "    # Calculamos el descriptor lo normalizamos.\n",
    "    descriptor = feature.hog(image, orientations=9, pixels_per_cell=(8, 8),  cells_per_block=(2, 2),\n",
    "                             transform_sqrt=True, visualise=False, block_norm=\"L2-Hys\")\n",
    "    descriptor_scaled = scaler.transform(descriptor.reshape(1, -1))\n",
    "    \n",
    "    # Usando nuestro clasificador inferimos que tipo de imagen se trata.\n",
    "    if clf.predict(descriptor_scaled) == 1:\n",
    "        ax1.set_title(\"Cara\")\n",
    "    else:\n",
    "        ax1.set_title(\"Otro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sliding windows y pirámide.\n",
    "Ahora sabemos que nuestro clasificador funciona, pero, ¿cómo hacemos para detectar las caras en una imagen corriente? A continuación se detallan los tres pasos necesarios para ello:\n",
    "\n",
    "* Generar una pirámide de imágenes con diferentes dimensiones.\n",
    "* Buscar caras en la imagen actual.\n",
    "* Eliminar los bbox que se solapan.\n",
    "\n",
    "### Generar la pirámide "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image = io.imread(\"resources/session2/our_face_dataset/test/test.png\", as_grey=True)\n",
    "\n",
    "# Calculamos la pirámide.\n",
    "pyramid = tuple(transform.pyramid_gaussian(image, max_layer=6, downscale=1.2))\n",
    "\n",
    "# Concatenar diferentes niveles para entender un poco mas. (No hace falta enteder)\n",
    "nlevels = 6\n",
    "row, col = image.shape\n",
    "montaje = np.zeros((row, col * nlevels))\n",
    "offset = 0\n",
    "for i in range(nlevels):\n",
    "    montaje[:pyramid[i].shape[0], (i * col) - offset:i * col - offset + pyramid[i].shape[1]] = pyramid[i]\n",
    "    offset += col - pyramid[i].shape[1]\n",
    "\n",
    "# Mostrar la pirámide.\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(18.5, 20.5, forward=True)\n",
    "plt.imshow(montaje, cmap=\"gray\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preguntas__\n",
    "* ¿Por qué tenemos que usar diferentes dimensiones de imagen?\n",
    "* ¿Cuantas veces reducimos la imagen?\n",
    "* ¿Para qué nos podría servir incrementar el tamaño de la imagen?\n",
    "\n",
    "### Sliding windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar las ventanas mòviles para el primer nivel de la pirámide.\n",
    "images = util.view_as_windows(pyramid[0], tamaño_redimensionar, step=16)\n",
    "print(\"Dimensiones de el objeto: %s.\" % str(images.shape))\n",
    "# Convertir en un array de imágenes\n",
    "images = images.reshape(images.shape[0] * images.shape[1], images.shape[2], images.shape[3])\n",
    "print(\"Dimensiones de el objeto: %s.\" % str(images.shape))\n",
    "\n",
    "# Generamos una figura con múltiples imágenes\n",
    "numero_de_slices = 6\n",
    "fig=plt.figure(figsize=(numero_de_slices, numero_de_slices))\n",
    "fig.set_size_inches(18.5, 20.5, forward=True)\n",
    "\n",
    "for i in range(1, (numero_de_slices * numero_de_slices) + 1):\n",
    "    \n",
    "    # Cargar imagen en grayscale y reescalar\n",
    "    image = images[i-1]\n",
    "    \n",
    "    # Mostrar la imagen\n",
    "    ax1 = fig.add_subplot(6, 6, i)\n",
    "    plt.imshow(image, cmap=\"gray\")\n",
    "    \n",
    "    # Calculamos el descriptor lo normalizamos\n",
    "    descriptor = feature.hog(image, orientations=9, pixels_per_cell=(8, 8),  cells_per_block=(2, 2),\n",
    "                             transform_sqrt=True, visualise=False, block_norm=\"L2-Hys\")\n",
    "    descriptor_scaled = scaler.transform(descriptor.reshape(1, -1))\n",
    "    \n",
    "    # Usando nuestro clasificado inferimos que tipo de imagen es\n",
    "    if clf.predict(descriptor_scaled) == 1:\n",
    "        ax1.set_title(\"Cara\")\n",
    "    else:\n",
    "        ax1.set_title(\"Otro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Preguntas__\n",
    "* ¿Que define el parámetro step en la función util.view_as_windows?\n",
    "\n",
    "# Todo junto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "step = 5 # 5-20\n",
    "downscale = 1.15 # 1.05-1.15\n",
    "max_layers = 4 # 3-15\n",
    "\n",
    "image = io.imread(\"resources/session2/our_face_dataset/test/test.png\", as_grey=True)\n",
    "pyramid = tuple(transform.pyramid_gaussian(image, max_layer=max_layers, downscale=downscale))\n",
    "\n",
    "# Level 0 - Imagen original.\n",
    "final_image = io.imread(\"resources/session2/our_face_dataset/test/test.png\")\n",
    "slices = util.view_as_windows(pyramid[0], tamaño_redimensionar, step=step)\n",
    "print(\"Numero de slices: %d\" % (slices.shape[0] * slices.shape[1]))\n",
    "bboxes = []\n",
    "for row in range(slices.shape[0]):\n",
    "    for col in range(slices.shape[1]):\n",
    "        \n",
    "        #Posiciones\n",
    "        slice_row_start= row * step\n",
    "        slice_col_start = col * step\n",
    "        slice_row_end= slice_row_start  + tamaño_redimensionar[0]\n",
    "        slice_col_end = slice_col_start  + tamaño_redimensionar[1]\n",
    "        lt_point = (slice_col_start, slice_row_start)\n",
    "        rb_point = (slice_col_end, slice_row_end)\n",
    "        bbox = (slice_col_start, slice_row_start, slice_col_end, slice_row_end)\n",
    "        \n",
    "        # Calcular el descriptor para la imagen.\n",
    "        descriptor = feature.hog(slices[row, col,:, :], orientations=9, pixels_per_cell=(8, 8),  cells_per_block=(2, 2),\n",
    "                             transform_sqrt=True, visualise=False, block_norm=\"L2-Hys\")\n",
    "        # Normalizarlo.\n",
    "        descriptor_scaled = scaler.transform(descriptor.reshape(1, -1))\n",
    "        \n",
    "        # Inferéncia\n",
    "        if clf.predict(descriptor_scaled) == 1:\n",
    "            cv2.rectangle(final_image, lt_point, rb_point, (0, 255, 0), 3)\n",
    "            bboxes.append(bbox)\n",
    "\n",
    "bboxes = np.array(bboxes)\n",
    "plt.figure(figsize=(50, 50))\n",
    "plt.imshow(final_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eliminar los bbox que se solapan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/, con comentarios traducidos.\n",
    "\n",
    "def non_max_suppression_fast(boxes, overlapThresh):\n",
    "    # Si no hay bbox devuelve una lista vacía\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    " \n",
    "    # Como vamos a calcular la solapación entre bbox nos interesa definir las\n",
    "    # posiciones con numeros flotantes para no perder precisión.\n",
    "    if boxes.dtype.kind == \"i\":\n",
    "        boxes = boxes.astype(\"float\")\n",
    " \n",
    "    # Inicializar la lista de bbox seleccionados\n",
    "    pick = []\n",
    " \n",
    "    # Inicializar arrays con las coordenadas de todos los bbox\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    " \n",
    "    # Calcular el area de todos los bbox y ordenarlos según el punto inferior derecho.\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    idxs = np.argsort(y2)\n",
    " \n",
    "    # Iterar mientras queden elementos en la idx\n",
    "    while len(idxs) > 0:\n",
    "        # Cojer el último elemento de la lista y añadirlo a la selección\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i)\n",
    " \n",
    "        # Buscar las coordenadas mas grandes del punto superior izquierdo\n",
    "        # y lo mismo para el punto inferior derecho.\n",
    "        xx1 = np.maximum(x1[i], x1[idxs[:last]])\n",
    "        yy1 = np.maximum(y1[i], y1[idxs[:last]])\n",
    "        xx2 = np.minimum(x2[i], x2[idxs[:last]])\n",
    "        yy2 = np.minimum(y2[i], y2[idxs[:last]])\n",
    " \n",
    "        # Calcular el ancho y alto del bbox.\n",
    "        w = np.maximum(0, xx2 - xx1 + 1)\n",
    "        h = np.maximum(0, yy2 - yy1 + 1)\n",
    " \n",
    "        # compute the ratio of overlap.\n",
    "        overlap = (w * h) / area[idxs[:last]]\n",
    " \n",
    "        # Eliminar del idx array todos los elementos que se sobrepasan el \n",
    "        # threshold especificado.\n",
    "        idxs = np.delete(idxs, np.concatenate(([last],\n",
    "            np.where(overlap > overlapThresh)[0])))\n",
    " \n",
    "    # Devolver los seleccionados volviendo a convertir las coordenadas en enteros.\n",
    "    return boxes[pick].astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Boxes antes de filtrar: %d\" % len(bboxes))\n",
    "bboxes_nm = non_max_suppression_fast(bboxes,0.05)\n",
    "print(\"Boxes antes de filtrar: %d\" % len(bboxes_nm))\n",
    "\n",
    "# Level 0 - Imagen original\n",
    "final_image = io.imread(\"resources/session2/our_face_dataset/test/test.png\")\n",
    "for bbox in bboxes_nm:\n",
    "    cv2.rectangle(final_image,(bbox[0],bbox[1]),(bbox[2], bbox[3]),(0,255,0),3)\n",
    "        \n",
    "plt.figure(figsize=(50, 50))\n",
    "plt.imshow(final_image);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenando un detector de caras de forma automática\n",
    "\n",
    "Debido a los recursos que tenemos en el momento de la presentación solo és possible entrenar un clasificador \"clasico\" y no uno con redes neuronales todo y que el metódo para hacerlo con esta libreria por ejemplo es muy similar a lo que haremos a continuación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definir nuestra clase de interés\n",
    "En la librería Dlib hay varias maneras de definir los bbox, os muestro esta por que es la mas senzilla y que en un caso real seria facil de usar. Pero en este proceso se puede ser creativo para ahorrar trabajo. No es sorprendente que si queremos etiquetar muchas imágenes usemos un \"método\" que no sea del todo para preciso para luego solo tener que refinar los resultados.\n",
    "\n",
    "## Formato XML\n",
    "\n",
    "### Usando el software de dlib o una web\n",
    "https://github.com/NaturalIntelligence/imglab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usando las anotaciones de las que ya disponemos\n",
    "En nuestro caso no tiene mucho sentido anotar otra vez las imágenes porque ya disponemos de esa información, por lo tanto lo único que tendremos que hacer es \"convertir\" el formato a xml para que dlib lo pueda interpretar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"\"\"\n",
    "<?xml version='1.0' encoding='ISO-8859-1'?>\n",
    "<?xml-stylesheet type='text/xsl' href='image_metadata_stylesheet.xsl'?>\n",
    "<dataset>\n",
    "    <name>Front looking faces for training dlib::get_frontal_face_detector()</name>\n",
    "    <comment></comment>\n",
    "    <images>\n",
    "        <image file='1\\a1.jpg'>\n",
    "            <box top='26' left='33' width='78' height='73'>\n",
    "              <label>Clase1</label>\n",
    "            </box>\n",
    "        </image>\n",
    "    <images>\n",
    "</dataset>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET\n",
    "import xml.dom.minidom as minidom\n",
    "\n",
    "def prettify(elem):\n",
    "    \"\"\"Return a pretty-printed XML string for the Element.\n",
    "    \"\"\"\n",
    "    rough_string = ET.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\"\\t\")\n",
    "\n",
    "\n",
    "root = ET.Element(\"dataset\")\n",
    "doc = ET.SubElement(root, \"name\").text = \"Dataset deteccion caras Curso Altran\"\n",
    "images = ET.SubElement(root, \"images\")\n",
    "\n",
    "for imagen in anotaciones:\n",
    "    nombre_de_archivo = imagen[0][0][0]\n",
    "    bboxes = imagen[1][0]\n",
    "    \n",
    "    imagen_actual = ET.SubElement(images, \"image\", file=nombre_de_archivo)\n",
    "    \n",
    "    for bbox in bboxes:\n",
    "        (x1, y1), (x2, y2) = np.array(bbox, dtype=int)\n",
    "        box = ET.SubElement(imagen_actual, \"box\", top=str(y1), left=str(x1), width=str(x2 - x1), height=str(y2 - y1))\n",
    "        box = ET.SubElement(imagen_actual, \"label\").text = \"cara\"\n",
    "xmlstr = minidom.parseString(ET.tostring(root)).toprettyxml(indent=\"   \")\n",
    "with open(\"resources/session2/face_dataset/custom_dataset.xml\", \"w\") as f:\n",
    "    f.write(xmlstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenando el clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do the training.  The train_simple_object_detector() function has a\n",
    "# bunch of options, all of which come with reasonable default values.  The next\n",
    "# few lines goes over some of these options.\n",
    "options = dlib.simple_object_detector_training_options()\n",
    "# Since faces are left/right symmetric we can tell the trainer to train a\n",
    "# symmetric detector.  This helps it get the most value out of the training\n",
    "# data.\n",
    "options.add_left_right_image_flips = False\n",
    "# The trainer is a kind of support vector machine and therefore has the usual\n",
    "# SVM C parameter.  In general, a bigger C encourages it to fit the training\n",
    "# data better but might lead to overfitting.  You must find the best C value\n",
    "# empirically by checking how well the trained detector works on a test set of\n",
    "# images you haven't trained on.  Don't just leave the value set at 5.  Try a\n",
    "# few different C values and see what works best for your data.\n",
    "options.C = 5\n",
    "options.epsilon = 0.001\n",
    "options.upsample_limit = 0\n",
    "\n",
    "# Tell the code how many CPU cores your computer has for the fastest training.\n",
    "options.num_threads = 2\n",
    "options.be_verbose = True\n",
    "\n",
    "training_xml_path = \"resources/session2/face_dataset/carles_training.xml\"\n",
    "testing_xml_path = \"resources/session2/face_dataset/carles_testing.xml\"\n",
    "\n",
    "# This function does the actual training.  It will save the final detector to\n",
    "# detector.svm.  The input is an XML file that lists the images in the training\n",
    "# dataset and also contains the positions of the face boxes.  To create your\n",
    "# own XML files you can use the imglab tool which can be found in the\n",
    "# tools/imglab folder.  It is a simple graphical tool for labeling objects in\n",
    "# images with boxes.  To see how to use it read the tools/imglab/README.txt\n",
    "# file.  But for this example, we just use the training.xml file included with\n",
    "# dlib.\n",
    "detector = dlib.train_simple_object_detector(training_xml_path, \"resources/session2/detector.svm\", options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training accuracy: {}\".format(\n",
    "    dlib.test_simple_object_detector(training_xml_path, \"resources/session2/detector.svm\")))\n",
    "print(\"Testing accuracy: {}\".format(\n",
    "    dlib.test_simple_object_detector(testing_xml_path, \"resources/session2/detector.svm\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.simple_object_detector(\"resources/session2/detector.svm\")\n",
    "win_det = dlib.image_window()\n",
    "win_det.set_image(detector)\n",
    "dlib.hit_enter_to_continue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final= io.imread(\"resources/session2/our_face_dataset/test/test.png\")\n",
    "detector = dlib.simple_object_detector(\"resources/session2/detector.svm\")\n",
    "    \n",
    "bboxes = detector(final, 1)\n",
    "print(\"number of rectangles found {}\".format(len(bboxes))) \n",
    "for i, bbox in enumerate(bboxes):\n",
    "    print(bbox)\n",
    "    cv2.rectangle(final,(bbox.left(),bbox.top()),(bbox.right(),bbox.bottom()),(0,255,0),3)\n",
    "        \n",
    "plt.figure(figsize=(50, 50))\n",
    "plt.imshow(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usando detectores ya entrenados\n",
    "\n",
    "## Dlib HoG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "final= io.imread(\"resources/session2/our_face_dataset/test/test.png\")\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "bboxes = detector(final, 1)\n",
    "print(\"number of rectangles found {}\".format(len(bboxes))) \n",
    "for i, bbox in enumerate(bboxes):\n",
    "    cv2.rectangle(final,(bbox.left(),bbox.top()),(bbox.right(),bbox.bottom()),(0,255,0),3)\n",
    "        \n",
    "plt.figure(figsize=(50, 50))\n",
    "plt.imshow(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dlib CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final= io.imread(\"resources/session2/our_face_dataset/test/test.png\")\n",
    "facerec = dlib.cnn_face_detection_model_v1(\"resources/session2/mmod_human_face_detector.dat\")\n",
    "bboxes = detector(final, 1)\n",
    "print(\"number of rectangles found {}\".format(len(bboxes))) \n",
    "for i, bbox in enumerate(bboxes):\n",
    "    cv2.rectangle(final,(bbox.left(),bbox.top()),(bbox.right(),bbox.bottom()),(0,255,0),3)\n",
    "        \n",
    "plt.figure(figsize=(50, 50))\n",
    "plt.imshow(final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
