{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignorar sklearn warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='skimage')\n",
    "\n",
    "# Librería de ámbito matemático, científico y de ingeniería. En nuestro caso, solo usamos la convolución.\n",
    "import scipy.io\n",
    "\n",
    "# Interfaz para HDF5. Solo necesario para cargar las anotaciones del dataset.\n",
    "import hdf5storage\n",
    "\n",
    "# Librería para la computación científica. En nuestro caso, la usamos por la representación de datos y hacer calculas de forma\n",
    "# rápida.\n",
    "import numpy as np\n",
    "\n",
    "# Librerías de visión por computador\n",
    "# OpenCV: Generalista.\n",
    "# Skimage: Generalista.\n",
    "# Dlib: Menos funcionalidades, muy rápido e implementaciones muy interesantes.\n",
    "import dlib\n",
    "import cv2\n",
    "from skimage import io\n",
    "from skimage import color\n",
    "from skimage import filters\n",
    "from skimage import util\n",
    "from skimage import transform\n",
    "from skimage import feature\n",
    "\n",
    "# Librerías para gráficos en 2d/3d. En nuestro caso para mostrar las imágenes y gráficos en general.\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Video\n",
    "\n",
    "# Necesario para visualizar los resultados en la misma página.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "Para comprender mejor las explicaciones de la presentación procederemos a realizar las siguientes tareas:\n",
    "\n",
    "* __Transformaciones afines__: Aplicaremos diferentes transformaciones afines y las usaremos en un ejemplo práctico.\n",
    "\n",
    "* __Proyectos__: En varios ejemplos enseño las diferentes utilidades de las transformaciones afines\n",
    "\n",
    "\n",
    "\n",
    "# Transformaciones afines\n",
    "\n",
    "## Translación \n",
    "\n",
    "El caso más sencillo es el de la translación, si recordamos de la parte teórica la matriz tiene la siguiente forma:\n",
    "\n",
    "![Traslacion](http://opencv-python-tutroals.readthedocs.io/en/latest/_images/math/22fe551f03b8e94f1a7a75731a660f0163030540.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos la imagen\n",
    "img = io.imread(\"resources/session1/altran.jpg\")\n",
    "filas, columnas, canales = img.shape\n",
    "\n",
    "# Definimos la matriz de traslación\n",
    "M = np.float32([[1,0,100],[0,1,50]])\n",
    "# Aplicamos la transformación\n",
    "dst = cv2.warpAffine(img,M,(columnas,filas))\n",
    "\n",
    "# Definimos la matriz de traslación\n",
    "M2 = np.float32([[1,0,5],[0,1,20]])\n",
    "# Aplicamos la transformación\n",
    "dst2 = cv2.warpAffine(img,M2,(columnas,filas))\n",
    "\n",
    "# Mostramos la imagen\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(\"131\")\n",
    "plt.imshow(img)\n",
    "plt.subplot(\"132\")\n",
    "plt.imshow(dst)\n",
    "plt.subplot(\"133\")\n",
    "plt.imshow(dst2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotación\n",
    "En este caso la matriz tiene la siguiente forma:\n",
    "\n",
    "![Imagen](http://opencv-python-tutroals.readthedocs.io/en/latest/_images/math/f3a6bed945808a1f3a9df71b260f68f8e653af95.png)\n",
    "\n",
    "pero en el caso de opencv se ha fijado de forma que el centro siga en el mismo lugar de la siguiente forma:\n",
    "\n",
    "![Opencv](http://opencv-python-tutroals.readthedocs.io/en/latest/_images/math/91ff2b9b1db0760f4764631010749e594cdf5f5f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = cv2.getRotationMatrix2D((columnas/2,filas/2),25,1)\n",
    "dst = cv2.warpAffine(img,M,(columnas * 2,filas * 2))\n",
    "\n",
    "# Mostramos la imagen\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(\"121\")\n",
    "plt.imshow(img)\n",
    "plt.subplot(\"122\")\n",
    "plt.imshow(dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación afín\n",
    "\n",
    "En la transformación afín, todas las líneas paralelas en la imagen original seguirán siendo paralelas en la imagen de salida. Para calcular la matriz de transformación, necesitamos localizar tres puntos en la imagen de entrada y sus ubicaciones también en la imagen de salida.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pts1 = np.float32([[50,50],[200,50],[50,200]])\n",
    "pts2 = np.float32([[40,60],[200,50],[75,225]])\n",
    "\n",
    "M = cv2.getAffineTransform(pts1,pts2)\n",
    "x_out = np.dot(M, [0,0,1]) - np.dot(M, [columnas,0,1])\n",
    "y_out = np.dot(M, [0,0,1]) - np.dot(M, [columnas,0,1])\n",
    "dst = cv2.warpAffine(img,M, (1300, 300), flags=cv2.INTER_CUBIC)\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(121),plt.imshow(img),plt.title('Input')\n",
    "plt.subplot(122),plt.imshow(dst),plt.title('Output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de perspectiva\n",
    "\n",
    "La transformación de perspectiva es ligeramente mas compleja que la transformación afín, en este caso podremos a traves de una matriz de 3x3 \"transformar\" una imágen de 3d a 2d.\n",
    "\n",
    "En este caso la matriz tiene la siguiente forma:\n",
    "![Imagen](https://scilab.io/wp-content/uploads/2017/08/persp-matrix-300x237.png)\n",
    "\n",
    "En el ejemplo a continuación usaremos la transformación perspectiva para obtener una visión frontal de un objeto en una escena cualquiera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab notebook\n",
    "\n",
    "img = io.imread(\"resources/session4/libro.jpg\")\n",
    "\n",
    "def onclick(event):\n",
    "    collector.append([event.xdata, event.ydata])\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "collector = []\n",
    "cid = fig.canvas.mpl_connect('button_press_event', onclick)\n",
    "\n",
    "plt.imshow(img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "print(collector)\n",
    "pts1 = np.float32(collector[0:4])\n",
    "pts2 = np.float32([[0,0],[300,0],[0,500],[300,500]])\n",
    "M = cv2.getPerspectiveTransform(pts1,pts2)\n",
    "\n",
    "dst = cv2.warpPerspective(img,M,(300,500))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(img)\n",
    "plt.subplot(122)\n",
    "plt.imshow(dst)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caso Práctico 1 - Aumentar número de muestras\n",
    "Uno de los usos más comunes de la transformaciones afines es usarlas para generar nuevas muestras en nuestro dataset a partir de ligeras transformaciones de imagenes conocidas, por ejemplo rotaciones, traslaciones, etc...\n",
    "\n",
    "En este ejemplo proponemos una función que nos permite generar tantas imágenes como necesitemos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image(img,ang_range,shear_range,trans_range):\n",
    "    '''\n",
    "    Esta funcion genera imágenes a partir de una sola imagen. Una distribución random uniforme \n",
    "    se usa para generar diferentes parameters para la transformación.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    img: np.array(2d)\n",
    "        Imagen inicial\n",
    "    ang_range: int\n",
    "        Rango de los ángulos para la rotación\n",
    "    shear_range: int\n",
    "        Rango de los valores usado para aplicar transformaciones afínes.\n",
    "    trans_range: int\n",
    "        Rango de los valores usados para aplicar translaciones\n",
    "    '''\n",
    "    \n",
    "    # Rotación\n",
    "    ang_rot = np.random.uniform(ang_range)-ang_range/2\n",
    "    rows,cols,ch = img.shape    \n",
    "    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)\n",
    "\n",
    "    # Translación\n",
    "    tr_x = trans_range*np.random.uniform()-trans_range/2\n",
    "    tr_y = trans_range*np.random.uniform()-trans_range/2\n",
    "    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])\n",
    "\n",
    "    # Shear\n",
    "    pts1 = np.float32([[5,5],[20,5],[5,20]])\n",
    "\n",
    "    pt1 = 5+shear_range*np.random.uniform()-shear_range/2\n",
    "    pt2 = 20+shear_range*np.random.uniform()-shear_range/2\n",
    "    \n",
    "    \n",
    "    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])\n",
    "\n",
    "    shear_M = cv2.getAffineTransform(pts1,pts2)\n",
    "        \n",
    "    img = cv2.warpAffine(img,Rot_M,(cols,rows))\n",
    "    img = cv2.warpAffine(img,Trans_M,(cols,rows))\n",
    "    img = cv2.warpAffine(img,shear_M,(cols,rows))\n",
    "    \n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image= io.imread(\"resources/session2/our_face_dataset/test/10.jpg\")\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "for i in range(100):\n",
    "    img = transform_image(image,20,10,5)\n",
    "    plt.subplot(10,10,i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso Práctico 2 - Eugenizer\n",
    "\n",
    "En este caso implementamos un puequeño ejemplo dónde se muestra como podemos transformar una imágen para que coincida con la perspectiva de una escena. En este caso cogeremos una imagen y la transforameros usando los landmarks detectados con el detector que entrenamos en la sessión anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image # Más facil manejar los PNG\n",
    "\n",
    "gafas = Image.open(\"resources/session4/gafas2.png\")\n",
    "puntos_gafas = [[20, 85], [200, 85], [280, 85], [485, 85]]\n",
    "plt.imshow(gafas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "facedetector = dlib.get_frontal_face_detector()\n",
    "facelandmark = dlib.shape_predictor('resources/session3/shape_predictor_68_face_landmarks.dat')\n",
    "facerembedder = dlib.face_recognition_model_v1(\"resources/session3/dlib_face_recognition_resnet_model_v1.dat\")\n",
    "\n",
    "# Datos de entrada\n",
    "input_video = cv2.VideoCapture('resources/session3/test.mp4')\n",
    "\n",
    "# Escojemos un frame al azar\n",
    "n_frames_used = np.random.randint(3200)\n",
    "for i in range(n_frames_used):\n",
    "    ret, frame = input_video.read()\n",
    "    \n",
    "# Lo convertimos a RGB\n",
    "dlib_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "frame = Image.fromarray(frame)\n",
    "frame = frame.convert('RGBA')\n",
    "# Detectamos las caras\n",
    "bboxes = facedetector(dlib_frame, 1)\n",
    "print(len(bboxes))\n",
    "# Iteramos sobre las caras encontradas\n",
    "for k, bbox in enumerate(bboxes):\n",
    "    \n",
    "    # Detectamos los landmarks\n",
    "    shape = facelandmark(dlib_frame, bbox)\n",
    "    \n",
    "    # Calculamos el angulo con los landmarks\n",
    "    \n",
    "    dX = shape.part(45).x - shape.part(36).x \n",
    "    dY = shape.part(45).y - shape.part(36).y \n",
    "    \n",
    "    angle = np.rad2deg(np.arctan2(dY, dX))\n",
    "    print(\"Size: \", dX, shape.part(41).y - shape.part(37).y)\n",
    "    gafas_tmp = gafas.resize((dX + 10, shape.part(41).y - shape.part(37).y  + 10))\n",
    "    gafas_tmp = gafas_tmp.rotate(-angle, expand=True)\n",
    "    frame.paste(gafas_tmp, (shape.part(37).x - 5, shape.part(39).y - 5), gafas_tmp)\n",
    "    \n",
    "# Mostramos la imágen original\n",
    "plt.figure(figsize=(50, 50));\n",
    "plt.imshow(frame);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3 - Panorama\n",
    "Para finalizar mostramos otra aplicación donde las trasformaciones afines se usan para poder generar panoramas usando múltiples imagenes. En este caso el procedimiento se basa en:\n",
    "\n",
    "* Buscar puntos de interés en las imágenes.\n",
    "* Correlacionar estos puntos.\n",
    "* Usar las correspondéncias para generar una transformación afín que nos permita transformar estos puntos de una escena a la otra.\n",
    "* Por ultimo transformar una de las imágenes con la matriz que hemos calculado previamente y combinar las dos imágenes para obtener el panorama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imageA = cv2.imread(\"resources/session4/panorama1.jpg\")\n",
    "imageB = cv2.imread(\"resources/session4/panorama2.jpg\")\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(\"121\")\n",
    "plt.imshow(imageA)\n",
    "plt.subplot(\"122\")\n",
    "plt.imshow(imageB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectAndDescribe(image):\n",
    "    \"\"\"\n",
    "    Extraer features y puntos de interés en una imágen.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    image : np.array(2d)\n",
    "        Imagen que queremos procesar.\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    (lst, lst)\n",
    "        Puntos de interés y features para la imagen.\n",
    "    \"\"\"\n",
    "    # Convertir la imagen a grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Usar el descriptor ORB\n",
    "    orb = cv2.ORB_create()\n",
    "    kps = orb.detect(gray)\n",
    "\n",
    "    # Extraer features de la imagen\n",
    "    (kps, features) = orb.compute(gray, kps)\n",
    "\n",
    "    return (kps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(kpsA, featuresA) = detectAndDescribe(imageA)\n",
    "(kpsB, featuresB) = detectAndDescribe(imageB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dibujamos los keypoints que hemos encontrado \n",
    "img1 = cv2.drawKeypoints(imageA, kpsA, None, color=(0,255,0), flags=0)\n",
    "img2 = cv2.drawKeypoints(imageB, kpsB, None, color=(0,255,0), flags=0)\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(\"121\")\n",
    "plt.imshow(img1)\n",
    "plt.subplot(\"122\")\n",
    "plt.imshow(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "matcher = cv2.DescriptorMatcher_create(\"BruteForce\")\n",
    "rawMatches = matcher.knnMatch(featuresA, featuresB, 2)\n",
    "matches = []\n",
    "\n",
    "# Este pequeño fragmento de código elimina los outliers\n",
    "for m in rawMatches:\n",
    "    # Lowe's ratio test\n",
    "    if len(m) == 2 and m[0].distance < m[1].distance * 0.75:\n",
    "        matches.append((m[0].trainIdx, m[0].queryIdx))\n",
    "\n",
    "# Convierte los Keypoints(OpenCV) a valores flotantes (np.array)\n",
    "kpsA = np.float32([kp.pt for kp in kpsA])\n",
    "kpsB = np.float32([kp.pt for kp in kpsB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Nos aseguramos que disponemos de un mínimo de 4 puntos para calcular la homografia A.K.A Transformación afín.\n",
    "if len(matches) > 4:\n",
    "    # Construimos los sets de puntos\n",
    "    ptsA = np.float32([kpsA[i] for (_, i) in matches])\n",
    "    ptsB = np.float32([kpsB[i] for (i, _) in matches])\n",
    "\n",
    "    # Computamos la homografia\n",
    "    (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC,4.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos la transformación y solapamos las imágenes\n",
    "result = cv2.warpPerspective(imageA, H, (imageB.shape[1] + imageA.shape[1], imageB.shape[0]))\n",
    "result[0:imageB.shape[0], 0:imageB.shape[1]] = imageB\n",
    "result = result[0:,0:400,:]\n",
    "\n",
    "# Mostramos el resultado\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.subplot(\"121\")\n",
    "plt.imshow(img1)\n",
    "plt.subplot(\"122\")\n",
    "plt.imshow(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
