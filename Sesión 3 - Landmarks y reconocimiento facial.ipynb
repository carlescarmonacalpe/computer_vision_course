{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignorar sklearn warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='skimage')\n",
    "\n",
    "# Librería de ámbito matemático, científico y de ingeniería. En nuestro caso, solo usamos la convolución.\n",
    "import scipy.io\n",
    "\n",
    "# Interfaz para HDF5. Solo necesario para cargar las anotaciones del dataset.\n",
    "import hdf5storage\n",
    "\n",
    "# Librería para la computación científica. En nuestro caso, la usamos por la representación de datos y hacer calculas de forma\n",
    "# rápida.\n",
    "import numpy as np\n",
    "\n",
    "# Librerías de visión por computador\n",
    "# OpenCV: Generalista.\n",
    "# Skimage: Generalista.\n",
    "# Dlib: Menos funcionalidades, muy rápido e implementaciones muy interesantes.\n",
    "import dlib\n",
    "import cv2\n",
    "from skimage import io\n",
    "from skimage import color\n",
    "from skimage import filters\n",
    "from skimage import util\n",
    "from skimage import transform\n",
    "from skimage import feature\n",
    "\n",
    "# Librerías para gráficos en 2d/3d. En nuestro caso para mostrar las imágenes y gráficos en general.\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from IPython.display import Video\n",
    "\n",
    "# Necesario para visualizar los resultados en la misma página.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducción\n",
    "\n",
    "Para comprender mejor las explicaciones de la presentación procederemos a realizar las siguientes tareas:\n",
    "\n",
    "* __Detector de landmarks__: Entrenaremos desde cero un detector de puntos de referencia en los rostros, y lo pondremos a prueba con una serie de imágenes y videos.\n",
    "\n",
    "* __Reconocimiento facial__: Usaremos un modelo entrenado para generar vectores que describan los rostros. Y con estos descriptores entrenaremos un clasificador que nos permitirá reconocer diferentes rostros, para finalizar probaremos el detector en un video.\n",
    "\n",
    "\n",
    "\n",
    "# Entrenando un detector de landmarks\n",
    "\n",
    "## Explorando el dataset\n",
    "\n",
    "Antes de empezar vamos a recordar de qué forma se estructuraba nuestro dataset para obtener las anotaciones necesarias para entrenar nuestro detector.\n",
    "\n",
    "Ruta del dataset:\n",
    "__resources\\face_dataset__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las anotaciones del dataset.\n",
    "anotaciones_path = \"resources/session2/face_dataset/anno.mat\"\n",
    "anotaciones = hdf5storage.loadmat(anotaciones_path)\n",
    "anotaciones = anotaciones[\"anno\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estructura de las anotaciones. Esta varia mucha según que de donde proviene el dataset.\n",
    "print(\"Datos de una imagen:\")\n",
    "print(\"-\"*20)\n",
    "print(\"Nombre del fichero: \", anotaciones[0][0][0])\n",
    "print(\"Bboxes:\\n %s\" % anotaciones[0][1][0][0])\n",
    "print(\"Rotación de la cara en el eje [X, Y, Z]:\\n %s\" % anotaciones[0][2][0][0])\n",
    "print(\"Landmarks:\\n %s\" % anotaciones[0][3][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero de muestras\n",
    "print(\"Numero de imagenes:\", len(anotaciones))\n",
    "print(\"Numero de caras anotadas:\", np.sum([len(anotacion[1][0]) for anotacion in anotaciones]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random # Generar números aleatorios\n",
    "\n",
    "# Seleccionar una imagen de forma aleatoria\n",
    "random_image = random.randint(0, (len(anotaciones)))\n",
    "filename = anotaciones[random_image][0][0][0]\n",
    "bboxs = anotaciones[random_image][1][0]\n",
    "landing_marks = anotaciones[random_image][3][0]\n",
    "\n",
    "# Estructura\n",
    "f, (ax0, ax1) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Abrir la imágen\n",
    "img = io.imread(\"resources/session3/face_dataset/\" + filename)\n",
    "ax0.imshow(img);\n",
    "ax0.set_title(\"Imagen sin anotar\")\n",
    "\n",
    "# Dibujar la información disponible\n",
    "for bbox, bbox_landing_marks in zip(bboxs, landing_marks):\n",
    "    # Rectangle\n",
    "    (x1, y1), (x2, y2) = bbox.astype(int)\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 3)\n",
    "    # Landmarks\n",
    "    for landmark in bbox_landing_marks:\n",
    "        x, y = landmark.astype(int)\n",
    "        cv2.circle(img, (x, y) , 7, (255, 0, 0), -1)\n",
    "        \n",
    "# Mostrar la imagen\n",
    "ax1.imshow(img);\n",
    "ax1.set_title(\"Imagen anotada\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generando los xml\n",
    "\n",
    "Tal y como hicimos en la última sesión necesitamos generar un fichero XML con un formato específico de para poder entrenar nuestro detector de landmarks."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "<?xml version='1.0' encoding='ISO-8859-1'?>\n",
    "<?xml-stylesheet type='text/xsl' href='image_metadata_stylesheet.xsl'?>\n",
    "<dataset>\n",
    "    <name>Front looking faces for training dlib::get_frontal_face_detector()</name>\n",
    "    <comment></comment>\n",
    "    <images>\n",
    "        <image file='1\\a1.jpg'>\n",
    "            <box top='26' left='33' width='78' height='73'>\n",
    "                <part name='00' x='277' y='194'/>\n",
    "                <part name='01' x='279' y='194'/>\n",
    "                <part name='02' x='277' y='193'/>\n",
    "                <part name='03' x='305' y='191'/>\n",
    "                <part name='04' x='305' y='194'/>\n",
    "                <part name='05' x='314' y='189'/>\n",
    "            </box>\n",
    "        </image>\n",
    "    <images>\n",
    "</dataset>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as ET # Necesario para generar el xml\n",
    "import xml.dom.minidom as minidom # Necesario para que poder formatear el xml\n",
    "\n",
    "def prettify(elem):\n",
    "    \"\"\"\n",
    "    Devuelve el xml con un formato de string estructurado.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "    elem : Et.Element\n",
    "        Árbol xml.\n",
    "    \"\"\"\n",
    "    rough_string = ET.tostring(elem, 'utf-8')\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    return reparsed.toprettyxml(indent=\"\\t\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_training_samples = 165\n",
    "\n",
    "# Training set\n",
    "root = ET.Element(\"dataset\")\n",
    "doc = ET.SubElement(root, \"name\").text = \"Dataset deteccion de landmarks Curso Altran - Training\"\n",
    "images = ET.SubElement(root, \"images\")\n",
    "\n",
    "for imagen in anotaciones[:165]:\n",
    "    nombre_de_archivo = imagen[0][0][0]\n",
    "    bboxes = imagen[1][0]\n",
    "    landing_marks = imagen[3][0]\n",
    "    \n",
    "    imagen_actual = ET.SubElement(images, \"image\", file=nombre_de_archivo)\n",
    "    \n",
    "    for bbox, bbox_landing_marks in zip(bboxes, landing_marks):\n",
    "        \n",
    "        # Rectangle\n",
    "        (x1, y1), (x2, y2) = bbox.astype(int)\n",
    "        box = ET.SubElement(imagen_actual, \"box\", top=str(y1), left=str(x1), width=str(x2 - x1), height=str(y2 - y1))\n",
    "        # Landmarks\n",
    "        for idx in range(len(bbox_landing_marks)):\n",
    "            x, y = bbox_landing_marks[idx].astype(int)\n",
    "            ET.SubElement(box, \"part\", name=str(idx), x=str(x), y=str(y))\n",
    "    \n",
    "xmlstr = minidom.parseString(ET.tostring(root)).toprettyxml(indent=\"   \")\n",
    "with open(\"resources/session3/face_dataset/training_dataset.xml\", \"w\") as f:\n",
    "    f.write(xmlstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "root = ET.Element(\"dataset\")\n",
    "doc = ET.SubElement(root, \"name\").text = \"Dataset deteccion de landmarks Curso Altran - Training\"\n",
    "images = ET.SubElement(root, \"images\")\n",
    "\n",
    "for imagen in anotaciones[:165]:\n",
    "    nombre_de_archivo = imagen[0][0][0]\n",
    "    bboxes = imagen[1][0]\n",
    "    landing_marks = imagen[3][0]\n",
    "    \n",
    "    imagen_actual = ET.SubElement(images, \"image\", file=nombre_de_archivo)\n",
    "    \n",
    "    for bbox, bbox_landing_marks in zip(bboxes, landing_marks):\n",
    "        \n",
    "        # Rectangle\n",
    "        (x1, y1), (x2, y2) = bbox.astype(int)\n",
    "        box = ET.SubElement(imagen_actual, \"box\", top=str(y1), left=str(x1), width=str(x2 - x1), height=str(y2 - y1))\n",
    "        # Landmarks\n",
    "        for idx in range(len(bbox_landing_marks)):\n",
    "            x, y = bbox_landing_marks[idx].astype(int)\n",
    "            ET.SubElement(box, \"part\", name=str(idx), x=str(x), y=str(y))\n",
    "    \n",
    "xmlstr = minidom.parseString(ET.tostring(root)).toprettyxml(indent=\"   \")\n",
    "with open(\"resources/session3/face_dataset/test_dataset.xml\", \"w\") as f:\n",
    "    f.write(xmlstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenar el dataset automaticamente\n",
    "\n",
    "Ahora que ya disponemos del dataset anotado en el formato que espera la librería Dlib, procedemos a entrenar nuestro detector con la implementación del proceso descrito en  __One Millisecond Face Alignment with an Ensemble of Regression Trees by\n",
    "Vahid Kazemi and Josephine Sullivan, CVPR 2014__.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = dlib.shape_predictor_training_options()\n",
    "\n",
    "# Este parámetro es muy útil si tenemos un dataset pequeño y queremos aumentar nuestros datos de entrenamiento.\n",
    "options.oversampling_amount = 1\n",
    "\n",
    "# Algunas opciones como el tree_depth o el nu se han escojido de tal forma que se reduce\n",
    "# la capacidad del detector, pero el tiempo de entrenamiento es mucho menor.\n",
    "options.num_threads = 3\n",
    "options.nu = 0.05\n",
    "options.tree_depth = 2\n",
    "options.be_verbose = True\n",
    "\n",
    "# Esta es la línea que ejecuta el entrenamiento y guarda los parámetros del modelo en el fichero especificado.\n",
    "training_xml_path = \"resources/session3/face_dataset/training_dataset.xml\"\n",
    "dlib.train_shape_predictor(training_xml_path, \"resources/session3/predictor.dat\", options)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medimos la accuracy con el dataset de entrenamiento\n",
    "print(\"\\nAccuracy con el dataset de entrenamiento: {}\".format(\n",
    "    dlib.test_shape_predictor(training_xml_path, \"resources/session3/landmark_detector.dat\")))\n",
    "\n",
    "# Medimos la accuracy con el dataset de test\n",
    "testing_xml_path = os.path.join(faces_folder, \"testing_with_face_landmarks.xml\")\n",
    "print(\"Accuracy con el dataset de test: {}\".format(\n",
    "    dlib.test_shape_predictor(testing_xml_path, \"predictor.dat\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados en una imagen\n",
    "### Resultados con nuestro detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos nuestra imagen de test favorita\n",
    "final= io.imread(\"resources/session2/our_face_dataset/test/test.png\")\n",
    "\n",
    "# Cargamos el detector de caras basado en CNN y el detector de landmarks\n",
    "facerec = dlib.cnn_face_detection_model_v1(\"resources/session2/mmod_human_face_detector.dat\")\n",
    "predictor = dlib.shape_predictor(\"resources/session3/predictor.dat\")\n",
    "\n",
    "#Inferimos los bbox con caras a partir del detector\n",
    "bboxes = detector(final, 1)\n",
    "print(\"Número de caras encontradas {}\".format(len(bboxes))) \n",
    "\n",
    "# Por cada cara encontrada en la imagen ejecutamos el detector de landmarks y mostramos el resultado\n",
    "for i, bbox in enumerate(bboxes):\n",
    "    cv2.rectangle(final,(bbox.left(),bbox.top()),(bbox.right(),bbox.bottom()),(0,255,0),3)\n",
    "    shape = predictor(img, d)\n",
    "    print(shape)\n",
    "    \n",
    "plt.figure(figsize=(50, 50))\n",
    "plt.imshow(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados con el detector de dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos nuestra imagen de test favorita\n",
    "final= io.imread(\"resources/session2/our_face_dataset/test/test.png\")\n",
    "\n",
    "# Cargamos el detector de caras basado en CNN y el detector de landmarks\n",
    "facedetect = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"resources/session3/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "#Inferimos los bbox con caras a partir del detector\n",
    "bboxes = facedetect(final, 1)\n",
    "print(\"Número de caras encontradas {}\".format(len(bboxes))) \n",
    "\n",
    "# Por cada cara encontrada en la imagen ejecutamos el detector de landmarks y mostramos el resultado\n",
    "for i, bbox in enumerate(bboxes):\n",
    "    cv2.rectangle(final,(bbox.left(),bbox.top()),(bbox.right(),bbox.bottom()),(0,255,0),3)\n",
    "    shape = predictor(final, bbox)\n",
    "    for num_part in range(shape.num_parts):\n",
    "        point = shape.part(num_part)\n",
    "        cv2.circle(final, (point.x, point.y) , 2, (255, 0, 0), -1)\n",
    "    \n",
    "plt.figure(figsize=(50, 50))\n",
    "plt.imshow(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resultados en un video\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos el objeto que nos permitira la lectura del vídeo\n",
    "input_video = cv2.VideoCapture('resources/session3/test.mp4')\n",
    "\n",
    "# Definimos el objeto que nos permitira la escritura del vídeo\n",
    "fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "output_video = cv2.VideoWriter('resources/session3/output.mp4',fourcc, 25.0, (640,480))\n",
    "\n",
    "# Cargamos el detector de caras basado en CNN y el detector de landmarks.\n",
    "facedetect = dlib.get_frontal_face_detector()\n",
    "facelandmarks = dlib.shape_predictor(\"resources/session3/shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# No sé por que OpenCV no detecta bien el final del video así que tenemos que usar estas variables extras.\n",
    "ret = True\n",
    "num_frame = 0\n",
    "\n",
    "# Mientras hay frames disponibles en el video de entrada\n",
    "while(input_video.isOpened() and ret):\n",
    "    ret, frame = input_video.read()\n",
    "    \n",
    "    if ret:\n",
    "        # Convertimos la imágen de BGR(OpenCV) a RGB(Dlib)\n",
    "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detectamos los rostros y dibujamos un bbox y círculos en los puntos de referéncia.\n",
    "        bboxes = facedetect(image_rgb, 1)\n",
    "        for i, bbox in enumerate(bboxes):\n",
    "            # Bbox\n",
    "            cv2.rectangle(image_rgb,(bbox.left(),bbox.top()),(bbox.right(),bbox.bottom()),(0,255,0),3)\n",
    "            # Landmarks\n",
    "            shape = facelandmarks(final, bbox)\n",
    "            for num_part in range(shape.num_parts):\n",
    "                point = shape.part(num_part)\n",
    "                cv2.circle(final, (point.x, point.y) , 1, (255, 0, 0), -1)\n",
    "        # Convertimos la imágen de RGB(Dlib) a BGR(OpenCV)        \n",
    "        image_rgb = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n",
    "        # Guardamos en el disco\n",
    "        output_video.write(image_rgb)\n",
    "        \n",
    "    # Mostrar frame\n",
    "    num_frame += 1\n",
    "    if num_frame % 100 == 0:\n",
    "        print(\"Actual frame: %s\" % (num_frame))\n",
    "        print(\"Num caras en el frame: %s\" % (len(bboxes)))\n",
    "        \n",
    "input_video.release()\n",
    "output_video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento facial\n",
    "Para realizar el reconocimiento facial tenemos que realizar básicamente los siguientes pasos:\n",
    "\n",
    "* Detectar caras\n",
    "* Normalizarlas/Alinearlas\n",
    "* Embbedirlas\n",
    "* Clasificarlas\n",
    "\n",
    "## Normalización / Alineación de los rostros\n",
    "\n",
    "Con este proceso intentamos normalizar las caras usando los landmarks, para ello aplicamos una transformación geométrica, que veremos en detalle en la siguiente sesión, y que nos permite centrar todos los rostros antes de calcular el vector embbedido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'resources/session3/')\n",
    "from align import AlignDlib\n",
    "\n",
    "# Inicializa la clase especificando que detector de landmarks usaremos\n",
    "alignment = AlignDlib('resources/session3/shape_predictor_68_face_landmarks.dat')\n",
    "facedetect = dlib.get_frontal_face_detector()\n",
    "\n",
    "# Abrimos la imagen\n",
    "image = io.imread(\"resources/session3/face_dataset/2404040793.jpg\")\n",
    "\n",
    "#Detectamos los rostros\n",
    "bboxes = facedetect(image, 1)\n",
    "\n",
    "# Alineación\n",
    "(image_aligned, landmarks) = alignment.align(80, image, bboxes[0], landmarkIndices=AlignDlib.INNER_EYES_AND_BOTTOM_LIP)\n",
    "\n",
    "# Imagen original\n",
    "plt.subplot(131)\n",
    "plt.imshow(image)\n",
    "\n",
    "# Recorte\n",
    "plt.subplot(132)\n",
    "bbox = bboxes[0]\n",
    "plt.imshow(image[bbox.top():bbox.bottom(), bbox.left():bbox.right()])\n",
    "\n",
    "# Imagen centrada\n",
    "plt.subplot(133)\n",
    "plt.imshow(image_aligned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculo del vector embedded\n",
    "\n",
    "Ahora toca convertir nuestras imágenes a un descriptor que define de la forma más certera los rostros. Este proceso es arduo de entrenar, ya que para conseguir unos resultados equivalentes a los que veremos ahora necesitaríamos una GPU de última generación, 3 millones de rostros anotados y más de 24 h de cómputo. Es por eso que vamos a usar una red preentrenada y simplemente vamos a usar los descriptores para clasificar los rostros que nosotros queramos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "facedetector = dlib.get_frontal_face_detector()\n",
    "facealignment = dlib.shape_predictor('resources/session3/shape_predictor_68_face_landmarks.dat')\n",
    "facerembedder = dlib.face_recognition_model_v1(\"resources/session3/dlib_face_recognition_resnet_model_v1.dat\")\n",
    "\n",
    "# Abrimos una imagen cualquiera\n",
    "image = io.imread(\"resources/session3/face_dataset/4553922208.jpg\")\n",
    "\n",
    "# Detectamos las caras\n",
    "bboxes = facedetector(image)\n",
    "\n",
    "# Iteramos sobre las caras encontradas y calculamos el vector\n",
    "for k, bbox in enumerate(bboxes):\n",
    "    landmarks = facealignment(image, bbox)\n",
    "    face_descriptor = facerembedder.compute_face_descriptor(image_aligned, landmarks)\n",
    "    print(\"Rostro %s : %s\" % (k, np.array(face_descriptor)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso aplicado\n",
    "\n",
    "En el video usado anteriormente buscaremos rostros en los primeros n frames, generaremos sus vectores y usaremos el famoso algoritmo PCA para reducir los vectores de 128 dimensiones a solo dos. Para finalizar mostraremos los descriptores en un gráfico y usaremos el algoritmo DBScan para encontrar los diferentes clúster y aproximar el número de caras diferentes que aparecen en el video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectores\n",
    "facedetector = dlib.get_frontal_face_detector()\n",
    "facelandmark = dlib.shape_predictor('resources/session3/shape_predictor_68_face_landmarks.dat')\n",
    "facerembedder = dlib.face_recognition_model_v1(\"resources/session3/dlib_face_recognition_resnet_model_v1.dat\")\n",
    "# Cargamos el video\n",
    "input_video = cv2.VideoCapture('resources/session3/test.mp4')\n",
    "n_frames_used = 400 # 400 / 25 = 16 segundos\n",
    "\n",
    "list_descriptores = []\n",
    "list_images = []\n",
    "\n",
    "print('Processing: ', end='')\n",
    "\n",
    "for num_frame in range(n_frames_used):\n",
    "    ret, frame = input_video.read()\n",
    "    # Convertimos la imágen de BGR(OpenCV) a RGB(Dlib)\n",
    "    image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Detectamos las caras\n",
    "    bboxes = facedetector(frame, 1)\n",
    "    # Iteramos sobre las caras encontradas y calculamos el vector\n",
    "    for k, bbox in enumerate(bboxes):\n",
    "        landmarks = facelandmark(frame, bbox)\n",
    "        face_descriptor = facerembedder.compute_face_descriptor(frame, landmarks)\n",
    "        list_descriptores.append(face_descriptor)\n",
    "        crop = frame[bbox.top():bbox.bottom(), bbox.left():bbox.right(), :]\n",
    "        list_images.append(transform.resize(crop, (45, 45)))\n",
    "        \n",
    "    if num_frame % 25 == 0:\n",
    "        print('.', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA # Algoritmo que nos permite describir un dataset con nuevas variables no correlacionadas\n",
    "from sklearn.cluster import DBSCAN # Algoritmo que permíte clusterizar los datos\n",
    "\n",
    "# Convertimos la lista de vectores en una matriz \n",
    "list_descriptores = np.array(list_descriptores)\n",
    "print(\"Dimensiones de la matrix: \", (list_descriptores.shape))\n",
    "\n",
    "# Reducimos las dimensiones\n",
    "pca = PCA(n_components=2).fit(list_descriptores)\n",
    "pca_2d = pca.transform(list_descriptores)\n",
    "\n",
    "# Buscamos clusters\n",
    "dbscan = DBSCAN(eps=0.1, min_samples=5)\n",
    "dbscan.fit(pca_2d)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos el gráfico con los resultados de DBSCAN\n",
    "plt.scatter(pca_2d[:, 0], pca_2d[:, 1], c=dbscan.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "artists = []\n",
    "for x, y, image in zip(pca_2d[:, 0], pca_2d[:, 1], list_images):\n",
    "    img = OffsetImage(image, zoom=0.6)\n",
    "    ab = AnnotationBbox(img, (x, y), xycoords='data', frameon=False)\n",
    "    ax.add_artist(ab)\n",
    "ax.set_xlim(-0.3, 0.5)\n",
    "ax.set_ylim(-0.3, 0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamos el clasificador\n",
    "\n",
    "Hasta este punto hemos visto cómo detectar los landmarks y como calcular un vector que describe un rostro, ¿cómo hacemos hora para reconocer cada rostro a cual corresponde?\n",
    "\n",
    "Básicamente entrenamos un clasificador parecido al que hicimos en la segunda sesión pero en este caso capaz de etiquetar múltiples categorías, una por persona que queramos reconocer.\n",
    "\n",
    "## Recolectando muestras\n",
    "\n",
    "El primer paso es generar muestras para nuestro clasificador, para ello usaremos los primeros n frames para detectar caras y guardar los crops de estos. Luego manualmente los divideremos en varios directorios una para cada uno de los actores que queremos reconocer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector = dlib.get_frontal_face_detector()\n",
    "face_landmark = dlib.shape_predictor('resources/session3/shape_predictor_68_face_landmarks.dat')\n",
    "face_embedder = dlib.face_recognition_model_v1(\"resources/session3/dlib_face_recognition_resnet_model_v1.dat\")\n",
    "input_video = cv2.VideoCapture('resources/session3/test.mp4')\n",
    "n_frames_utilitzados = 900\n",
    "\n",
    "list_descriptores = []\n",
    "\n",
    "num_box = 0\n",
    "for num_frame in range(n_frames_utilitzados):\n",
    "    # Leer \n",
    "    ret, frame = input_video.read()\n",
    "    # Convertimos la imágen de BGR(OpenCV) a RGB(Dlib)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB).astype(np.uint8)\n",
    "    # Detectamos las caras\n",
    "    bboxes = facedetector(frame, 1)\n",
    "    for bbox in bboxes:\n",
    "        io.imsave(\"resources/session3/our_dataset/unannotated/\" + str(num_box) + \".jpg\",\n",
    "                  frame[bbox.top():bbox.bottom(), bbox.left():bbox.right(), :])\n",
    "        num_box += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embbeded vector\n",
    "\n",
    "Ahora que ya tenemos los diferentes personajes a reconocer divididos en carpetas, procedemos a calcular los descriptores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "facelandmark = dlib.shape_predictor('resources/session3/shape_predictor_68_face_landmarks.dat')\n",
    "facerembedder = dlib.face_recognition_model_v1(\"resources/session3/dlib_face_recognition_resnet_model_v1.dat\")\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "nombre_carpeta = \"resources/session3/our_dataset/david/\"\n",
    "for filename in glob.glob(nombre_carpeta + '*.jpg'):\n",
    "    # Cargar muestra\n",
    "    image = io.imread(filename)\n",
    "    landmarks = facelandmark(image, dlib.rectangle(0, 0, image.shape[0], image.shape[1]))\n",
    "    face_descriptor = facerembedder.compute_face_descriptor(image, landmarks)\n",
    "    X.append(face_descriptor)\n",
    "    Y.append(1)\n",
    "    \n",
    "nombre_carpeta = \"resources/session3/our_dataset/emma/\"\n",
    "for filename in glob.glob(nombre_carpeta + '*.jpg'):\n",
    "    # Cargar muestra\n",
    "    image = io.imread(filename)\n",
    "    landmarks = facelandmark(image, dlib.rectangle(0, 0, image.shape[0], image.shape[1]))\n",
    "    face_descriptor = facerembedder.compute_face_descriptor(image, landmarks)\n",
    "    X.append(face_descriptor)\n",
    "    Y.append(2)\n",
    "    \n",
    "nombre_carpeta = \"resources/session3/our_dataset/lopez/\"\n",
    "for filename in glob.glob(nombre_carpeta + '*.jpg'):\n",
    "    # Cargar muestra\n",
    "    image = io.imread(filename)\n",
    "    landmarks = facelandmark(image, dlib.rectangle(0, 0, image.shape[0], image.shape[1]))\n",
    "    face_descriptor = facerembedder.compute_face_descriptor(image, landmarks)\n",
    "    X.append(face_descriptor)\n",
    "    Y.append(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar un clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X)\n",
    "descriptors_scaled = scaler.transform(X)\n",
    "\n",
    "# Dividimos el dataeset en dos partes entrenamiento/test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esto es un CTRL-C + CTRL-V de los tutorial de Sklearn: \n",
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html#sphx-glr-auto-examples-model-selection-plot-grid-search-digits-py\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4, 1e-5],\n",
    "                     'C': [1, 10, 100, 1000]}]#,\n",
    "                    #{'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,\n",
    "                       scoring='%s_macro' % score, n_jobs=4)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_names = {1: \"David\", 2: \"Emma\", 3: \"Lopez\"}\n",
    "facedetector = dlib.get_frontal_face_detector()\n",
    "# facedetector = dlib.cnn_face_detection_model_v1(\"resources/session2/mmod_human_face_detector.dat\")\n",
    "facelandmark = dlib.shape_predictor('resources/session3/shape_predictor_68_face_landmarks.dat')\n",
    "facerembedder = dlib.face_recognition_model_v1(\"resources/session3/dlib_face_recognition_resnet_model_v1.dat\")\n",
    "input_video = cv2.VideoCapture('resources/session3/test.mp4')\n",
    "\n",
    "# Escojemos un frame al azar\n",
    "n_frames_used = np.random.randint(3200)\n",
    "for i in range(n_frames_used):\n",
    "    ret, frame = input_video.read()\n",
    "    \n",
    "# Lo convertimos a RGB\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "# Detectamos las caras\n",
    "bboxes = facedetector(frame, 1)\n",
    "# Iteramos sobre las caras encontradas\n",
    "for k, bbox in enumerate(bboxes):\n",
    "    # Calculamos el decriptor\n",
    "    landmarks = facelandmark(frame, bbox)\n",
    "    face_descriptor = np.array(facerembedder.compute_face_descriptor(frame, landmarks)).reshape(1, -1)\n",
    "    # Lo normalizamos\n",
    "    descriptors_scaled = scaler.transform(face_descriptor)\n",
    "    # Inferimos el personaje con nuestro clasificador\n",
    "    label = clf.predict(descriptors_scaled)\n",
    "    # Dibujamos un bbox y su nombre\n",
    "    cv2.rectangle(frame,(bbox.left(), bbox.top()), (bbox.right(), bbox.bottom()), (255, 0, 0) )\n",
    "    cv2.putText(frame, \"#\" + labels_names[label[0]], (bbox.left(), bbox.top() - 5), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0), 1)\n",
    "    \n",
    "# Mostramos la imágen original\n",
    "plt.figure(figsize=(50, 50));\n",
    "plt.imshow(frame);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
